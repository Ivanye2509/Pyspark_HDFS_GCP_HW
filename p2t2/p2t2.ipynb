{"cells":[{"cell_type":"code","execution_count":1,"id":"6361956a","metadata":{},"outputs":[],"source":["import os\n","os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.12:0.14.0 pyspark-shell'\n","import re\n","import sys\n","from operator import add\n","from typing import Iterable,Tuple\n","from pyspark.resultiterable import ResultIterable\n","from pyspark.sql import SparkSession\n","from pyspark.sql.types import StructType, StructField, IntegerType, StringType,FloatType\n","from pyspark.streaming import StreamingContext"]},{"cell_type":"code","execution_count":9,"id":"d3d360f1","metadata":{"scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/02 18:05:45 WARN org.apache.spark.streaming.StreamingContext: Dynamic Allocation is enabled for this application. Enabling Dynamic allocation for Spark Streaming applications can cause data loss if Write Ahead Log is not enabled for non-replayable sources. See the programming guide for details on how to enable the Write Ahead Log.\n","22/05/02 18:05:46 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514746000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:47 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514747000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:48 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514748000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:49 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514749000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:50 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514750000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:51 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514751000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:52 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514752000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514753000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514754000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:55 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514755000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:56 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514756000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:57 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514757000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:58 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514758000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:05:59 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514759000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514760000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:01 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514761000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:02 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514762000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:03 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514763000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:04 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514764000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:05 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514765000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:06 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514766000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:07 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514767000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:08 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514768000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:09 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514769000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:10 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514770000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:11 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514771000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:12 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514772000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:13 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514773000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:14 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514774000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","[Stage 0:>                                                          (0 + 1) / 1]\r"]}],"source":["## Emitter Part(start pagerank algorithm right before start this chunk using p1t3q8.ipynb)\n","\n","spark = SparkSession.builder.getOrCreate()\n","\n","scc = StreamingContext(spark.sparkContext,1)\n","emitter_stream = scc.textFileStream(\"gs://csee4121homework/outputs/p2t2_whole.csv/\")\n","\n","def function(emitter_stream):\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","                    .toDF(['_c0', '_c1'])\\\n","                    .coalesce(1).write.option(\"delimiter\", \"\\t\")\\\n","                    .mode(\"append\").csv(\"gs://csee4121homework/outputs/p2t2_emitter/\")\n","emitter_stream.foreachRDD(function)\n","scc.start()"]},{"cell_type":"code","execution_count":10,"id":"310ede7e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/02 18:06:16 WARN org.apache.spark.sql.streaming.StreamingQueryManager: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"]},{"data":{"text/plain":["<pyspark.sql.streaming.StreamingQuery at 0x7f9638bdc340>"]},"execution_count":10,"metadata":{},"output_type":"execute_result"},{"name":"stderr","output_type":"stream","text":["22/05/02 18:06:32 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error generating jobs for time 1651514792000 ms\n","org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: gs://csee4121homework/outputs/p2t2_whole.csv/_SUCCESS\n","\tat org.apache.hadoop.mapred.LocatedFileStatusFetcher.getFileStatuses(LocatedFileStatusFetcher.java:153)\n","\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:280)\n","\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:396)\n","\tat org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:131)\n","\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\n","\tat scala.Option.getOrElse(Option.scala:189)\n","\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.$anonfun$filesToRDD$1(FileInputDStream.scala:289)\n","\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n","\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n","\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n","\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\n","\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n","\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n","\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.org$apache$spark$streaming$dstream$FileInputDStream$$filesToRDD(FileInputDStream.scala:279)\n","\tat org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:152)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$3(DStream.scala:343)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$2(DStream.scala:343)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.DStream.$anonfun$getOrCompute$1(DStream.scala:342)\n","\tat scala.Option.orElse(Option.scala:447)\n","\tat org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:335)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)\n","\tat org.apache.spark.streaming.DStreamGraph.$anonfun$generateJobs$2(DStreamGraph.scala:123)\n","\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\n","\tat scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:75)\n","\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\n","\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\n","\tat scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)\n","\tat org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:122)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.$anonfun$generateJobs$1(JobGenerator.scala:252)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:250)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:186)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:91)\n","\tat org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:90)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","22/05/02 18:06:32 WARN org.apache.spark.sql.catalyst.util.ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n","22/05/02 18:06:41 WARN org.apache.spark.sql.catalyst.util.ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n","22/05/02 18:06:48 WARN org.apache.spark.sql.catalyst.util.ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n","22/05/02 18:06:49 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514778000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:50 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 16.0 (TID 16) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","22/05/02 18:06:52 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 16.0 failed 4 times; aborting job\n","22/05/02 18:06:52 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 0899f8f6-1e4d-45c5-aabb-077ffb9e11b6.\n","org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 4 times, most recent failure: Lost task 0.3 in stage 16.0 (TID 22) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 16.0 (TID 17) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): TaskKilled (Stage cancelled)\n","22/05/02 18:06:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 3.0 in stage 16.0 (TID 19) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): TaskKilled (Stage cancelled)\n","22/05/02 18:06:52 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 16.0 (TID 18) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): TaskKilled (Stage cancelled)\n","22/05/02 18:06:52 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514779000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 1372, in csv\n","    self._jwrite.csv(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n","    return f(*a, **kw)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o3863.csv.\n",": org.apache.spark.SparkException: Job aborted.\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n","\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n","\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\n","\tat sun.reflect.GeneratedMethodAccessor151.invoke(Unknown Source)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 4 times, most recent failure: Lost task 0.3 in stage 16.0 (TID 22) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:52 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514780000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:06:54 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 18.0 (TID 25) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","22/05/02 18:07:01 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 18.0 failed 4 times; aborting job\n","22/05/02 18:07:01 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 9cd17816-f1a7-421a-ab41-74626caf29f3.\n","org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 4 times, most recent failure: Lost task 1.3 in stage 18.0 (TID 29) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:01 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514781000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 1372, in csv\n","    self._jwrite.csv(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n","    return f(*a, **kw)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o3903.csv.\n",": org.apache.spark.SparkException: Job aborted.\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n","\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n","\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\n","\tat sun.reflect.GeneratedMethodAccessor151.invoke(Unknown Source)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 18.0 failed 4 times, most recent failure: Lost task 1.3 in stage 18.0 (TID 29) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:07 WARN org.apache.spark.sql.catalyst.util.ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n","22/05/02 18:07:13 WARN org.apache.spark.sql.catalyst.util.ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n","22/05/02 18:07:19 WARN org.apache.spark.sql.catalyst.util.ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n","22/05/02 18:07:20 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 34.0 (TID 44) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","22/05/02 18:07:23 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 34.0 failed 4 times; aborting job\n","22/05/02 18:07:23 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 1083ed0f-8ec9-465a-b7b3-73651ce4ad9f.\n","org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 48) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:23 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514785000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 1372, in csv\n","    self._jwrite.csv(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n","    return f(*a, **kw)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o4030.csv.\n",": org.apache.spark.SparkException: Job aborted.\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n","\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n","\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\n","\tat sun.reflect.GeneratedMethodAccessor151.invoke(Unknown Source)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 4 times, most recent failure: Lost task 0.3 in stage 34.0 (TID 48) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:31 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514787000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:32 WARN org.apache.spark.sql.catalyst.util.ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n","22/05/02 18:07:33 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 41.0 (TID 55) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","22/05/02 18:07:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 41.0 failed 4 times; aborting job\n","22/05/02 18:07:37 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 248c427a-b649-4ad9-bbc1-f349175ce1df.\n","org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 4 times, most recent failure: Lost task 0.3 in stage 41.0 (TID 61) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:37 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514788000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 1372, in csv\n","    self._jwrite.csv(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n","    return f(*a, **kw)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o4101.csv.\n",": org.apache.spark.SparkException: Job aborted.\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n","\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n","\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\n","\tat sun.reflect.GeneratedMethodAccessor151.invoke(Unknown Source)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 41.0 failed 4 times, most recent failure: Lost task 0.3 in stage 41.0 (TID 61) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:44 WARN org.apache.spark.sql.catalyst.util.ParseMode: overwrite is not a valid parse mode. Using PERMISSIVE.\n","22/05/02 18:07:44 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 2.0 in stage 48.0 (TID 68) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","22/05/02 18:07:46 ERROR org.apache.spark.scheduler.TaskSetManager: Task 2 in stage 48.0 failed 4 times; aborting job\n","22/05/02 18:07:46 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 5ae0e3bd-af9c-49f8-9331-fef509ffe28f.\n","org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 48.0 failed 4 times, most recent failure: Lost task 2.3 in stage 48.0 (TID 72) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:46 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 48.0 (TID 66) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): TaskKilled (Stage cancelled)\n","22/05/02 18:07:46 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514790000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 1372, in csv\n","    self._jwrite.csv(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n","    return f(*a, **kw)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o4166.csv.\n",": org.apache.spark.SparkException: Job aborted.\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n","\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n","\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\n","\tat sun.reflect.GeneratedMethodAccessor151.invoke(Unknown Source)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 48.0 failed 4 times, most recent failure: Lost task 2.3 in stage 48.0 (TID 72) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:48 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 51.0 (TID 74) (cluster-3c25-w-0.c.csee-4121-hw.internal executor 1): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","22/05/02 18:07:53 ERROR org.apache.spark.scheduler.TaskSetManager: Task 0 in stage 51.0 failed 4 times; aborting job\n","22/05/02 18:07:53 ERROR org.apache.spark.sql.execution.datasources.FileFormatWriter: Aborting job 51664e90-ce92-4d13-a599-146dd296c080.\n","org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 4 times, most recent failure: Lost task 0.3 in stage 51.0 (TID 77) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514791000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/readwriter.py\", line 1372, in csv\n","    self._jwrite.csv(path)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1304, in __call__\n","    return_value = get_return_value(\n","  File \"/usr/lib/spark/python/pyspark/sql/utils.py\", line 111, in deco\n","    return f(*a, **kw)\n","  File \"/opt/conda/miniconda3/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n","    raise Py4JJavaError(\n","py4j.protocol.Py4JJavaError: An error occurred while calling o4200.csv.\n",": org.apache.spark.SparkException: Job aborted.\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n","\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n","\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n","\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n","\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n","\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n","\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n","\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\n","\tat sun.reflect.GeneratedMethodAccessor151.invoke(Unknown Source)\n","\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.lang.reflect.Method.invoke(Method.java:498)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n","\tat java.lang.Thread.run(Thread.java:750)\n","Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 51.0 failed 4 times, most recent failure: Lost task 0.3 in stage 51.0 (TID 77) (cluster-3c25-w-1.c.csee-4121-hw.internal executor 2): java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2259)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2208)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2207)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2207)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2446)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2388)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2377)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","Caused by: java.lang.IllegalStateException: Input row doesn't have expected number of values required by the schema. 2 fields are required while 3 values are provided.\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$$nestedInanonfun$makeFromJava$16$1.applyOrElse(EvaluatePython.scala:186)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.nullSafeConvert(EvaluatePython.scala:211)\n","\tat org.apache.spark.sql.execution.python.EvaluatePython$.$anonfun$makeFromJava$16(EvaluatePython.scala:180)\n","\tat org.apache.spark.sql.SparkSession.$anonfun$applySchemaToPythonRDD$2(SparkSession.scala:750)\n","\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n","\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n","\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n","\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n","\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n","\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:157)\n","\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n","\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514793000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514794000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514795000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514796000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514797000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514798000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514799000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514800000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514801000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514802000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514803000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514804000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514805000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514806000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514807000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514808000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514809000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514810000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514811000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514812000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514813000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514814000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514815000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514816000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514817000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514818000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514819000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514820000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514821000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514822000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514823000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514824000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514825000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514826000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514827000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514828000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514829000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514830000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514831000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514832000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514833000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514834000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514835000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514836000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514837000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514838000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514839000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514840000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514841000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514842000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514843000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514844000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514845000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514846000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514847000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514848000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514849000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514850000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514851000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514852000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514853000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514854000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514855000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514856000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514857000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514858000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514859000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514860000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514861000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514862000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514863000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514864000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514865000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514866000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514867000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514868000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514869000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514870000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514871000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514872000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514873000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514874000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:55 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514875000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:56 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514876000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:57 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514877000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:58 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514878000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:07:59 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514879000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514880000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:01 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514881000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:02 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514882000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:03 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514883000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:04 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514884000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:05 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514885000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:06 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514886000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:07 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514887000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:08 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514888000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:09 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514889000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:10 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514890000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:11 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514891000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:12 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514892000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:13 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514893000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:14 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514894000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:15 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514895000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:16 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514896000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:17 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514897000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:18 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514898000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:19 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514899000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:20 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514900000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:21 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514901000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:22 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514902000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:23 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514903000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:24 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514904000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:25 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514905000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:26 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514906000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:27 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514907000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:28 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514908000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:29 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514909000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:30 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514910000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:31 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514911000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:32 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514912000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:33 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514913000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:34 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514914000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:35 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514915000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:36 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514916000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:37 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514917000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:38 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514918000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:39 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514919000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:40 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514920000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:41 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514921000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:42 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514922000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:43 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514923000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:44 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514924000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:45 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514925000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:46 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514926000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:47 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514927000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:48 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514928000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:49 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514929000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:50 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514930000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:51 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514931000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:52 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514932000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514933000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514934000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:55 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514935000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:56 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514936000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:57 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514937000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:58 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514938000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:08:59 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514939000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514940000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:01 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514941000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:02 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514942000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:02 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 8 file(s) in 2454 ms\n","22/05/02 18:09:03 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514943000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:04 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514944000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:05 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514945000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:06 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514946000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:07 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514947000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:08 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514948000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:09 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514949000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:10 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514950000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:11 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514951000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:12 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514952000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:13 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514953000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:14 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514954000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:15 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514955000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:16 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514956000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:17 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514957000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:18 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514958000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:19 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514959000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:20 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514960000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:21 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514961000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:22 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514962000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:23 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514963000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:24 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514964000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:25 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514965000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:26 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514966000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:27 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514967000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:28 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514968000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:29 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514969000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:30 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514970000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:30 WARN org.apache.spark.sql.execution.streaming.FileStreamSource: Listed 8 file(s) in 3183 ms\n","22/05/02 18:09:31 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514971000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:32 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514972000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:33 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514973000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:34 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514974000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:35 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514975000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:36 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514976000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:37 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514977000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:38 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514978000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:39 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514979000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:40 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514980000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:41 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514981000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:42 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514982000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:43 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514983000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:44 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514984000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:45 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514985000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:46 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514986000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:47 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514987000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:48 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514988000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:49 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514989000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:50 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514990000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:51 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514991000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:52 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514992000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:53 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514993000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:54 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514994000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:55 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514995000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:56 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514996000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:57 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514997000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:58 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514998000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:09:59 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651514999000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:00 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515000000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:01 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515001000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:02 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515002000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:03 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515003000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:04 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515004000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:05 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515005000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:06 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515006000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:07 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515007000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:08 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515008000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:09 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515009000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:10 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515010000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:11 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515011000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:12 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515012000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:13 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515013000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:14 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515014000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:15 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515015000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:16 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515016000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:17 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515017000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:18 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515018000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:19 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515019000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:20 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515020000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:21 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515021000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:22 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515022000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:23 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515023000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:24 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515024000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:25 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515025000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:26 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515026000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:27 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515027000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:28 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515028000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:29 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515029000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:30 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515030000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:31 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515031000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:32 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515032000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:33 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515033000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:34 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515034000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:35 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515035000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:36 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515036000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:37 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515037000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:38 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515038000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:39 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515039000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:40 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515040000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:41 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515041000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:42 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515042000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:43 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515043000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:44 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515044000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n","22/05/02 18:10:45 ERROR org.apache.spark.streaming.scheduler.JobScheduler: Error running job streaming job 1651515045000 ms.0\n","org.apache.spark.SparkException: An exception was raised by Python:\n","Traceback (most recent call last):\n","  File \"/usr/lib/spark/python/pyspark/streaming/util.py\", line 68, in call\n","    r = self.func(t, *rdds)\n","  File \"/usr/lib/spark/python/pyspark/streaming/dstream.py\", line 155, in <lambda>\n","    func = lambda t, rdd: old_func(rdd)\n","  File \"/tmp/ipykernel_723/2416125911.py\", line 9, in function\n","    df = emitter_stream.map(lambda content: content.split('\\t'))\\\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 66, in toDF\n","    return sparkSession.createDataFrame(self, schema, sampleRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 675, in createDataFrame\n","    return self._create_dataframe(data, schema, samplingRatio, verifySchema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 698, in _create_dataframe\n","    rdd, schema = self._createFromRDD(data.map(prepare), schema, samplingRatio)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 486, in _createFromRDD\n","    struct = self._inferSchema(rdd, samplingRatio, names=schema)\n","  File \"/usr/lib/spark/python/pyspark/sql/session.py\", line 460, in _inferSchema\n","    first = rdd.first()\n","  File \"/usr/lib/spark/python/pyspark/rdd.py\", line 1589, in first\n","    raise ValueError(\"RDD is empty\")\n","ValueError: RDD is empty\n","\n","\tat org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:95)\n","\tat org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.api.python.PythonDStream$.$anonfun$callForeachRDD$1$adapted(PythonDStream.scala:179)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:417)\n","\tat org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.Try$.apply(Try.scala:213)\n","\tat org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:256)\n","\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n","\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n","\tat org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:256)\n","\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n","\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n","\tat java.lang.Thread.run(Thread.java:750)\n"]}],"source":["## Receiver Part\n","spark = SparkSession.builder.getOrCreate()\n","userSchema =  StructType([ StructField(\"_c0\", StringType(), True),\\\n","                          StructField(\"_c1\", FloatType(), True)])\n","\n","stream = spark.readStream \\\n","    .option(\"delimiter\", \"\\t\") \\\n","    .schema(userSchema) \\\n","    .csv(\"gs://csee4121homework/outputs/p2t2_emitter/\") \n","\n","receiver_path_checkpoint = 'gs://csee4121homework/outputs/whole_checkpoint/'\n","\n","path2 = 'gs://csee4121homework/outputs/whole_receiver_from_emitter/'\n","\n","receiver = stream\\\n","    .select(\"_c0\", \"_c1\")\\\n","    .where(\"_c1 > 0.5\")\\\n","    .coalesce(1)\\\n","    .writeStream \\\n","    .format(\"csv\") \\\n","    .option(\"mode\",\"overwrite\")\\\n","    .option(\"delimiter\", \"\\t\") \\\n","    .option(\"checkpointLocation\", receiver_path_checkpoint)\\\n","    .option(\"path\", path2) \\\n","    \n","receiver.start()"]},{"cell_type":"code","execution_count":12,"id":"382ce2e8","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["22/05/02 18:10:47 WARN org.apache.spark.streaming.StreamingContext: StreamingContext has already been stopped\n"]}],"source":["scc.stop()"]},{"cell_type":"code","execution_count":14,"id":"c277ad6a","metadata":{"scrolled":true},"outputs":[],"source":["receiver.stop()"]},{"cell_type":"code","execution_count":20,"id":"f43fe242","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of articles in the database has a rank greater than 0.5 for part 2 task 1: 1762190\n","Number of articles in the database has a rank greater than 0.5 for part 2 task 2: 1762190\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["spark = SparkSession.builder.getOrCreate()\n","print(\"Number of articles in the database has a rank greater than 0.5 for part 2 task 1:\",\\\n","      spark.read.csv(\"gs://csee4121homework/outputs/whole_receiver\",sep = \"\\t\").count())\n","print(\"Number of articles in the database has a rank greater than 0.5 for part 2 task 2:\",\\\n","      spark.read.csv(\"gs://csee4121homework/outputs/whole_receiver_from_emitter\",sep = \"\\t\").repartition(1).count())"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"}},"nbformat":4,"nbformat_minor":5}